{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "852ec727",
   "metadata": {},
   "source": [
    "<h2>Install model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6be97e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/anaconda3/lib/python3.11/site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow_hub in /opt/anaconda3/lib/python3.11/site-packages (0.16.1)\n",
      "Requirement already satisfied: tensorflow-macos==2.15.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.65.5)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow_hub) (2.15.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.15.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.34.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dafabdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.__version__ 2.15.0\n",
      "hub.__version__ 0.16.1\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "print(\"tf.__version__\", tf.__version__)\n",
    "print(\"hub.__version__\", hub.__version__)\n",
    "\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a4cb2f",
   "metadata": {},
   "source": [
    "<H2>Define model</H2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b13900db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Model: mobilenet_v2_100_224 https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\n",
      "Input size (224, 224)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"mobilenet_v2_100_224\"\n",
    "\n",
    "model_handle_map = {\n",
    "  \"efficientnetv2-s\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-s-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-xl-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2\",\n",
    "  \"efficientnetv2-b0-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/feature_vector/2\",\n",
    "  \"efficientnetv2-s-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-xl-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/feature_vector/2\",\n",
    "  \"efficientnetv2-b0-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/feature_vector/2\",\n",
    "  \"efficientnetv2-b0\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/feature_vector/2\",\n",
    "  \"efficientnet_b0\": \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\",\n",
    "  \"efficientnet_b1\": \"https://tfhub.dev/tensorflow/efficientnet/b1/feature-vector/1\",\n",
    "  \"efficientnet_b2\": \"https://tfhub.dev/tensorflow/efficientnet/b2/feature-vector/1\",\n",
    "  \"efficientnet_b3\": \"https://tfhub.dev/tensorflow/efficientnet/b3/feature-vector/1\",\n",
    "  \"efficientnet_b4\": \"https://tfhub.dev/tensorflow/efficientnet/b4/feature-vector/1\",\n",
    "  \"efficientnet_b5\": \"https://tfhub.dev/tensorflow/efficientnet/b5/feature-vector/1\",\n",
    "  \"efficientnet_b6\": \"https://tfhub.dev/tensorflow/efficientnet/b6/feature-vector/1\",\n",
    "  \"efficientnet_b7\": \"https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1\",\n",
    "  \"bit_s-r50x1\": \"https://tfhub.dev/google/bit/s-r50x1/1\",\n",
    "  \"inception_v3\": \"https://tfhub.dev/google/imagenet/inception_v3/feature-vector/4\",\n",
    "  \"inception_resnet_v2\": \"https://tfhub.dev/google/imagenet/inception_resnet_v2/feature-vector/4\",\n",
    "  \"resnet_v1_50\": \"https://tfhub.dev/google/imagenet/resnet_v1_50/feature-vector/4\",\n",
    "  \"resnet_v1_101\": \"https://tfhub.dev/google/imagenet/resnet_v1_101/feature-vector/4\",\n",
    "  \"resnet_v1_152\": \"https://tfhub.dev/google/imagenet/resnet_v1_152/feature-vector/4\",\n",
    "  \"resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4\",\n",
    "  \"resnet_v2_101\": \"https://tfhub.dev/google/imagenet/resnet_v2_101/feature-vector/4\",\n",
    "  \"resnet_v2_152\": \"https://tfhub.dev/google/imagenet/resnet_v2_152/feature-vector/4\",\n",
    "  \"nasnet_large\": \"https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/4\",\n",
    "  \"nasnet_mobile\": \"https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4\",\n",
    "  \"pnasnet_large\": \"https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/4\",\n",
    "  \"mobilenet_v2_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\",\n",
    "  \"mobilenet_v2_130_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/feature_vector/4\",\n",
    "  \"mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/4\",\n",
    "  \"mobilenet_v3_small_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_small_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_large_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_large_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5\",\n",
    "}\n",
    "\n",
    "model_image_size_map = {\n",
    "  \"efficientnetv2-s\": 384,\n",
    "  \"efficientnetv2-m\": 480,\n",
    "  \"efficientnetv2-l\": 480,\n",
    "  \"efficientnetv2-b0\": 224,\n",
    "  \"efficientnetv2-b1\": 240,\n",
    "  \"efficientnetv2-b2\": 260,\n",
    "  \"efficientnetv2-b3\": 300,\n",
    "  \"efficientnetv2-s-21k\": 384,\n",
    "  \"efficientnetv2-m-21k\": 480,\n",
    "  \"efficientnetv2-l-21k\": 480,\n",
    "  \"efficientnetv2-xl-21k\": 512,\n",
    "  \"efficientnetv2-b0-21k\": 224,\n",
    "  \"efficientnetv2-b1-21k\": 240,\n",
    "  \"efficientnetv2-b2-21k\": 260,\n",
    "  \"efficientnetv2-b3-21k\": 300,\n",
    "  \"efficientnetv2-s-21k-ft1k\": 384,\n",
    "  \"efficientnetv2-m-21k-ft1k\": 480,\n",
    "  \"efficientnetv2-l-21k-ft1k\": 480,\n",
    "  \"efficientnetv2-xl-21k-ft1k\": 512,\n",
    "  \"efficientnetv2-b0-21k-ft1k\": 224,\n",
    "  \"efficientnetv2-b1-21k-ft1k\": 240,\n",
    "  \"efficientnetv2-b2-21k-ft1k\": 260,\n",
    "  \"efficientnetv2-b3-21k-ft1k\": 300, \n",
    "  \"efficientnet_b0\": 224,\n",
    "  \"efficientnet_b1\": 240,\n",
    "  \"efficientnet_b2\": 260,\n",
    "  \"efficientnet_b3\": 300,\n",
    "  \"efficientnet_b4\": 380,\n",
    "  \"efficientnet_b5\": 456,\n",
    "  \"efficientnet_b6\": 528,\n",
    "  \"efficientnet_b7\": 600,\n",
    "  \"inception_v3\": 299,\n",
    "  \"inception_resnet_v2\": 299,\n",
    "  \"nasnet_large\": 331,\n",
    "  \"pnasnet_large\": 331,\n",
    "}\n",
    "\n",
    "model_handle = model_handle_map.get(model_name)\n",
    "pixels = model_image_size_map.get(model_name, 224)\n",
    "\n",
    "print(f\"Selected Model: {model_name} {model_handle}\")\n",
    "\n",
    "IMAGE_SIZE = (pixels, pixels)\n",
    "\n",
    "print(f\"Input size {IMAGE_SIZE}\")\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6e796b",
   "metadata": {},
   "source": [
    "<h2>Define dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d2f2347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
      "228813984/228813984 [==============================] - 21s 0us/step\n"
     ]
    }
   ],
   "source": [
    "data_dir = tf.keras.utils.get_file(\n",
    "        'flower_photos',\n",
    "        'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "        untar=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3647d72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 files belonging to 5 classes.\n",
      "Using 2936 files for training.\n",
      "train_ds before <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 5), dtype=tf.float32, name=None))>\n",
      "class_names ('daisy', 'dandelion', 'roses', 'sunflowers', 'tulips')\n",
      "train_size 2936\n",
      "train_ds <_RepeatDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 5), dtype=tf.float32, name=None))>\n",
      "train_size 2936\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(subset):\n",
    "  return tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      data_dir,\n",
    "      validation_split=.20,\n",
    "      subset=subset,\n",
    "      label_mode=\"categorical\",\n",
    "      # Seed needs to provided when using validation_split and shuffle = True.\n",
    "      # A fixed seed is used so that the validation set is stable across runs.\n",
    "      seed=123,\n",
    "      image_size=IMAGE_SIZE,\n",
    "      batch_size=1)\n",
    "\n",
    "train_ds = build_dataset(\"training\")\n",
    "class_names = tuple(train_ds.class_names)\n",
    "print(\"train_ds before\", train_ds)\n",
    "train_size = train_ds.cardinality().numpy()\n",
    "# right now each batch has single image \n",
    "# [1] [2] [3] [4] [5] [6]\n",
    "# un batch them\n",
    "# 1 2 3 4 5 6\n",
    "# and group them in batch of 16 as BATCH_SIZE = 16\n",
    "# [1 2 3 4 5 6]\n",
    "train_ds = train_ds.unbatch().batch(BATCH_SIZE)\n",
    "train_ds = train_ds.repeat()\n",
    "\n",
    "print(\"class_names\", class_names)\n",
    "print(\"train_size\", train_size)\n",
    "print(\"train_ds\", train_ds)\n",
    "print(\"train_size\", train_size)\n",
    "\n",
    "# till here we have only imported the training data set \n",
    "# in single batch and re imported them in batch of 16\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ad8403",
   "metadata": {},
   "source": [
    "1. build_dataset(subset)\n",
    "    This function is designed to load a dataset of images from a directory, with specific settings for training and validation splits.\n",
    "\n",
    "    - tf.keras.preprocessing.image_dataset_from_directory:\n",
    "        This function loads images from a directory, where the images are organized into subdirectories, with each subdirectory representing a class label.\n",
    "    - data_dir: This is the directory where the image data is stored.\n",
    "    - validation_split=.20: 20% of the data will be set aside for validation.\n",
    "    - subset=subset: This argument will be either \"training\" or \"validation\", determining whether the function returns the training or validation dataset.\n",
    "    - label_mode=\"categorical\": The labels for the images will be returned in categorical form (one-hot encoded).\n",
    "    - seed=123: This is the seed for shuffling and splitting the data. It ensures that the dataset split remains consistent across multiple runs.\n",
    "    - image_size=IMAGE_SIZE: The size to which all images will be resized. This should be a tuple, e.g., (224, 224).\n",
    "    - batch_size=1: The dataset will be loaded with a batch size of 1 initially.\n",
    "    \n",
    "2. train_ds = build_dataset(\"training\")\n",
    "    - This line calls the build_dataset function with \"training\" as the subset, meaning it will load the training portion of the dataset.\n",
    "3. class_names = tuple(train_ds.class_names)\n",
    "    - train_ds.class_names gives the list of class names (the subdirectory names under data_dir). This line converts the list of class names into a tuple.\n",
    "4. train_size = train_ds.cardinality().numpy()\n",
    "    - train_ds.cardinality() returns the number of batches in the train_ds dataset. Since the batch size was initially set to 1, this effectively gives the total number of training images. Calling .numpy() converts the result from a TensorFlow tensor to a regular Python number.\n",
    "5. train_ds = train_ds.unbatch().batch(BATCH_SIZE)\n",
    "    - train_ds.unbatch(): This \"unbatches\" the dataset, essentially flattening it so that each individual image is treated as a separate batch.\n",
    "    - .batch(BATCH_SIZE): This rebatches the dataset using a new BATCH_SIZE, which is likely much larger than 1. This step is necessary because the dataset was initially loaded with a batch size of 1, but training will typically require larger batch sizes.\n",
    "6. train_ds = train_ds.repeat()\n",
    "    - This line repeats the dataset indefinitely. This is often done in training pipelines where the number of epochs (full passes over the dataset) is controlled outside the dataset itself, rather than by the dataset size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d203d52",
   "metadata": {},
   "source": [
    "<h3>Repeat data set example</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1548ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a simple dataset with elements [1, 2, 3]\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "\n",
    "# Repeat the dataset indefinitely\n",
    "repeated_dataset = dataset.repeat()\n",
    "\n",
    "# Take the first 10 elements from the repeated dataset to see how it repeats\n",
    "for element in repeated_dataset.take(10):\n",
    "    print(element.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0817010",
   "metadata": {},
   "source": [
    "<h3>Prefetch data set example</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9134a361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1\n",
      "Processing 2\n",
      "1\n",
      "2Processing 3\n",
      "\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# Create a simple dataset with elements [1, 2, 3]\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "\n",
    "# Map a function to add a delay to simulate a time-consuming operation\n",
    "def slow_map(x):\n",
    "    tf.print(\"Processing\", x)\n",
    "    # Use tf.py_function to wrap the sleep function\n",
    "    tf.py_function(lambda: time.sleep(1), [], [])\n",
    "    return x\n",
    "\n",
    "# Apply the slow_map function and then prefetch 1 element\n",
    "prefetched_dataset = dataset.map(slow_map).prefetch(1)\n",
    "\n",
    "# Iterate over the dataset\n",
    "for element in prefetched_dataset:\n",
    "    print(element.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c4078f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1. / 255)\n",
    "preprocessing_model = tf.keras.Sequential([normalization_layer])\n",
    "\n",
    "# do_data_augmentation=False:\n",
    "# The model will be trained on the original images after they are normalized.\n",
    "# This might be appropriate when you want to evaluate the model’s performance \n",
    "# on the raw data without any synthetic changes.\n",
    "\n",
    "# do_data_augmentation=True:\n",
    "# The model will be trained on augmented images, \n",
    "# where each image might be randomly rotated, translated, zoomed, \n",
    "# or flipped. This can help the model generalize better by learning \n",
    "# from slightly altered versions of the same images.\n",
    "do_data_augmentation = False\n",
    "if do_data_augmentation:\n",
    "  preprocessing_model.add(\n",
    "      tf.keras.layers.RandomRotation(40))\n",
    "  preprocessing_model.add(\n",
    "      tf.keras.layers.RandomTranslation(0, 0.2))\n",
    "  preprocessing_model.add(\n",
    "      tf.keras.layers.RandomTranslation(0.2, 0))\n",
    "  # Like the old tf.keras.preprocessing.image.ImageDataGenerator(),\n",
    "  # image sizes are fixed when reading, and then a random zoom is applied.\n",
    "  # If all training inputs are larger than image_size, one could also use\n",
    "  # RandomCrop with a batch size of 1 and rebatch later.\n",
    "  preprocessing_model.add(\n",
    "      tf.keras.layers.RandomZoom(0.2, 0.2))\n",
    "  preprocessing_model.add(\n",
    "      tf.keras.layers.RandomFlip(mode=\"horizontal\"))\n",
    "\n",
    "#  lambda function for declairing single line function\n",
    "# add = lambda x, y: x + y\n",
    "# result = add(5, 3)\n",
    "\n",
    "train_ds = train_ds.map(lambda images, labels:\n",
    "                        (preprocessing_model(images), labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08e681f",
   "metadata": {},
   "source": [
    "Let's break down the code snippet you've provided, which involves preprocessing a dataset in TensorFlow, with an option to perform data augmentation.\n",
    "\n",
    "### 1. **Normalization Layer**\n",
    "\n",
    "```python\n",
    "normalization_layer = tf.keras.layers.Rescaling(1. / 255)\n",
    "```\n",
    "\n",
    "- **Purpose:** This layer scales the pixel values of images from the range `[0, 255]` to `[0, 1]`. This normalization is a common preprocessing step that helps the model train more effectively by standardizing the input values.\n",
    "- **Operation:** Every pixel value in the image is divided by 255. This makes the pixel values fall within the range of `[0, 1]`.\n",
    "\n",
    "### 2. **Sequential Model for Preprocessing**\n",
    "\n",
    "```python\n",
    "preprocessing_model = tf.keras.Sequential([normalization_layer])\n",
    "```\n",
    "\n",
    "- **Purpose:** A `tf.keras.Sequential` model is created, and the normalization layer is added to it. This model will be used to apply preprocessing steps to the dataset.\n",
    "- **Operation:** Initially, this model contains only the normalization layer, but more layers can be added later based on the need for data augmentation.\n",
    "\n",
    "### 3. **Data Augmentation Setup**\n",
    "\n",
    "```python\n",
    "do_data_augmentation = False\n",
    "if do_data_augmentation:\n",
    "    preprocessing_model.add(tf.keras.layers.RandomRotation(40))\n",
    "    preprocessing_model.add(tf.keras.layers.RandomTranslation(0, 0.2))\n",
    "    preprocessing_model.add(tf.keras.layers.RandomTranslation(0.2, 0))\n",
    "    preprocessing_model.add(tf.keras.layers.RandomZoom(0.2, 0.2))\n",
    "    preprocessing_model.add(tf.keras.layers.RandomFlip(mode=\"horizontal\"))\n",
    "```\n",
    "\n",
    "- **Purpose:** This part of the code conditionally adds data augmentation layers to the `preprocessing_model`.\n",
    "- **Data Augmentation:** \n",
    "  - **RandomRotation:** Rotates images randomly within a specified range (in this case, up to 40 degrees).\n",
    "  - **RandomTranslation:** Shifts images randomly in the vertical and horizontal directions by a specified fraction of the total image size.\n",
    "  - **RandomZoom:** Zooms into or out of the image randomly by a specified fraction.\n",
    "  - **RandomFlip:** Flips the images horizontally randomly.\n",
    "- **Condition:** The `do_data_augmentation` flag controls whether these augmentation layers are added. If `do_data_augmentation` is set to `True`, the augmentation layers will be added; otherwise, they won't.\n",
    "\n",
    "### 4. **Mapping the Preprocessing Model to the Dataset**\n",
    "\n",
    "```python\n",
    "train_ds = train_ds.map(lambda images, labels: (preprocessing_model(images), labels))\n",
    "```\n",
    "\n",
    "- **Purpose:** This line applies the `preprocessing_model` to each image in the training dataset.\n",
    "- **Mapping Function:**\n",
    "  - The `map()` function applies the transformation to each batch of images in the dataset. Here, it uses a lambda function to pass the images through the `preprocessing_model`.\n",
    "  - **Images:** The lambda function takes each batch of images and applies the `preprocessing_model`, which could include normalization and, if enabled, data augmentation.\n",
    "  - **Labels:** The labels are passed through unchanged (`labels` are the same after the mapping function).\n",
    "\n",
    "### Why This Approach?\n",
    "\n",
    "1. **Modular Preprocessing:** The use of a `tf.keras.Sequential` model for preprocessing is modular and easy to extend. You can add or remove preprocessing steps as needed without modifying much code.\n",
    "\n",
    "2. **Conditional Data Augmentation:** The flag `do_data_augmentation` allows you to easily toggle data augmentation on or off. This can be useful for experimentation, such as comparing the performance of models trained with and without augmentation.\n",
    "\n",
    "3. **Efficiency:** By applying all preprocessing steps (including data augmentation, if enabled) within the `map()` function, the transformations are applied as part of the data pipeline. This is efficient and integrates well with TensorFlow’s data processing capabilities.\n",
    "\n",
    "4. **Flexibility:** The approach allows for complex preprocessing and augmentation pipelines, which can significantly improve model robustness by training it on a variety of altered images.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "- **Normalization:** Scales image pixel values to a range `[0, 1]`.\n",
    "- **Data Augmentation:** Applies random transformations (rotation, translation, zoom, flip) to the images if enabled.\n",
    "- **Pipeline Integration:** The preprocessing and augmentation steps are applied as part of the data pipeline, ensuring that they are efficiently processed during training. \n",
    "\n",
    "This setup allows you to flexibly preprocess and augment your training data, potentially improving the generalization ability of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb92b14a",
   "metadata": {},
   "source": [
    "<h3>Layer</h3>\n",
    "\n",
    "### What is a Layer?\n",
    "\n",
    "A **layer** is like a building block of a neural network. Think of a neural network as a machine made up of several layers stacked together. Each layer does a specific job to help the machine learn and make decisions.\n",
    "\n",
    "### Types of Layers\n",
    "\n",
    "There are different types of layers, each with a different job:\n",
    "\n",
    "1. **Dense Layer (Fully Connected Layer):**\n",
    "   - This layer is like a web where every point is connected to every other point. It helps the network understand the input data by connecting all the pieces together.\n",
    "   - Imagine this as a big net catching all the data points and connecting them.\n",
    "   - Example: `tf.keras.layers.Dense(units, activation=None)` where units is the number of neurons in the layer.\n",
    "\n",
    "2. **Convolutional Layer:**\n",
    "   - This layer looks at small parts of the input, like scanning an image bit by bit to find patterns, like edges or textures.\n",
    "   - Think of this like a magnifying glass moving over a picture, focusing on small details.\n",
    "   - Example: `tf.keras.layers.Conv2D(filters, kernel_size)` where filters is the number of convolutional filters, and kernel_size is the size of the filter.\n",
    "\n",
    "3. **Pooling Layer:**\n",
    "   - This layer simplifies the data by picking the most important parts and shrinking the rest. It helps make the network faster and prevents it from getting confused by too much detail.\n",
    "   - Imagine you’re summarizing a long story by just picking out the key points.\n",
    "   - Example: `tf.keras.layers.MaxPooling2D(pool_size)` where pool_size is the size of the pooling window.\n",
    "\n",
    "\n",
    "4. **Recurrent Layer:**\n",
    "   - This layer remembers things from previous steps. It’s useful for tasks like predicting the next word in a sentence because it keeps track of what came before.\n",
    "   - Think of it like your memory while you’re reading—remembering what you just read helps you understand what comes next.\n",
    "   - Example: `tf.keras.layers.LSTM(units)` where units is the number of units in the LSTM layer.\n",
    "\n",
    "5. **Normalization Layer:**\n",
    "   - This layer adjusts the data to make everything more consistent, which makes learning easier for the network.\n",
    "   - It's like making sure all your ingredients are measured out perfectly before baking a cake.\n",
    "   - Example: `tf.keras.layers.BatchNormalization()` or `tf.keras.layers.Rescaling(scale_factor)`.\n",
    "\n",
    "6. **Dropout Layer:**\n",
    "   - This layer randomly ignores some parts of the data to help the network not overthink things. This can make it better at generalizing to new data.\n",
    "   - It’s like practicing a sport with one hand tied behind your back to get better overall.\n",
    "   - Example: `tf.keras.layers.Dropout(rate)` where rate is the fraction of input units to drop.\n",
    "\n",
    "\n",
    "7. **Activation Layer:**\n",
    "   - This layer adds some \"spark\" or decision-making power to the network by applying a mathematical function. It helps the network learn more complex things.\n",
    "   - Think of it as flipping a switch that helps the network decide if something is important or not.\n",
    "   - Example: `tf.keras.layers.Activation('relu')` where 'relu' is the type of activation function.\n",
    "\n",
    "\n",
    "### How Layers Work Together\n",
    "\n",
    "- **Input Layer:** The first layer that takes in the raw data, like feeding in a picture or some numbers.\n",
    "- **Hidden Layers:** These layers do the hard work of learning from the data. They process the input and look for patterns.\n",
    "- **Output Layer:** The last layer that gives the final answer or prediction, like saying \"this is a cat\" or \"this is a dog.\"\n",
    "\n",
    "### Example in Real Life\n",
    "\n",
    "Imagine you’re building a machine that sorts fruits:\n",
    "\n",
    "1. **Input Layer:** You put a picture of a fruit into the machine.\n",
    "2. **Hidden Layers:** The machine looks at the picture closely, checks the color, shape, and size, and tries to recognize patterns.\n",
    "3. **Output Layer:** The machine finally says, \"This is an apple.\"\n",
    "\n",
    "### In Summary\n",
    "\n",
    "- **Layer:** A part of a neural network that does a specific job, like looking at details, making connections, or making decisions.\n",
    "- **Types:** Different layers do different things, like focusing on details, simplifying data, or remembering past information.\n",
    "- **Together:** Layers work together in a sequence to take raw data and turn it into something useful, like a prediction or a decision.\n",
    "\n",
    "Layers are what make a neural network smart, by processing the data step by step until it can understand and make decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf29c01e",
   "metadata": {},
   "source": [
    "# Deciding which types of layers\n",
    "\n",
    "Deciding which types of layers to combine for a specific task in a neural network depends on the nature of the task and the type of data you're working with. Here’s a simplified guide to help you make these decisions:\n",
    "\n",
    "### 1. **Understand the Task and Data Type**\n",
    "\n",
    "The first step is to understand the task you want to solve and the type of data you have. Here are some common scenarios:\n",
    "\n",
    "- **Image Classification/Recognition:** You want to classify images into categories (e.g., identifying whether a photo contains a cat or a dog).\n",
    "- **Text Processing/Sequence Prediction:** You need to process sequences of text, like predicting the next word in a sentence or understanding the sentiment of a text.\n",
    "- **Regression/Prediction:** You want to predict a continuous value, like house prices or temperatures.\n",
    "\n",
    "### 2. **Choose Layers Based on Data Type**\n",
    "\n",
    "- **For Image Data (e.g., Image Classification):**\n",
    "  - **Convolutional Layers (Conv2D):** These are essential for processing images. They help the network understand spatial patterns like edges, textures, and shapes by focusing on small regions of the image.\n",
    "  - **Pooling Layers (MaxPooling2D):** These reduce the size of the image data while keeping the most important information. This makes the network faster and less likely to overfit.\n",
    "  - **Dense Layers:** After the convolutional layers, dense layers help in making the final decision by connecting all the extracted features and combining them to classify the image.\n",
    "\n",
    "  **Example:**\n",
    "  - **Conv2D + Conv2D + MaxPooling2D + Dense**\n",
    "\n",
    "- **For Text Data (e.g., Sentiment Analysis, Translation):**\n",
    "  - **Embedding Layer:** This layer is used to convert words into vectors of numbers that capture the meaning or relationship between words. It's often the first layer in text-based models.\n",
    "  - **Recurrent Layers (LSTM, GRU):** These layers are good for handling sequences of data, such as sentences or time series, because they remember information from previous steps in the sequence.\n",
    "  - **Dense Layers:** These are used after the recurrent layers to make the final prediction (e.g., positive or negative sentiment).\n",
    "\n",
    "  **Example:**\n",
    "  - **Embedding + LSTM + Dense**\n",
    "\n",
    "- **For Numerical Data (e.g., Predicting House Prices):**\n",
    "  - **Dense Layers:** For tasks like regression, where you predict a continuous value, dense layers are commonly used. You might start with a few dense layers to learn different levels of abstraction from the input data.\n",
    "  - **Normalization Layer:** If your input data has a wide range of values, you might normalize it to help the network learn more effectively.\n",
    "\n",
    "  **Example:**\n",
    "  - **Dense + Dense + Dense**\n",
    "\n",
    "### 3. **Decide on the Number of Layers**\n",
    "\n",
    "- **Shallow vs. Deep Networks:** \n",
    "  - If your problem is simple (e.g., linear relationships), you might only need a few layers.\n",
    "  - For more complex tasks (e.g., image recognition with many categories), deeper networks with more layers can learn more complex patterns.\n",
    "\n",
    "### 4. **Experiment and Adjust**\n",
    "\n",
    "- **Start Simple:** Begin with a basic architecture that makes sense for your task and data.\n",
    "- **Experiment:** Try adding or removing layers, changing the type of layers, or adjusting the number of units in each layer.\n",
    "- **Tune Hyperparameters:** Adjust the learning rate, batch size, and other hyperparameters to optimize performance.\n",
    "\n",
    "### Examples of Common Architectures:\n",
    "\n",
    "- **Convolutional Neural Networks (CNNs):** Used for image tasks.\n",
    "  - **Architecture:** Conv2D -> Conv2D -> MaxPooling -> Conv2D -> MaxPooling -> Dense -> Output\n",
    "- **Recurrent Neural Networks (RNNs) or LSTMs/GRUs:** Used for sequence tasks like text or time series.\n",
    "  - **Architecture:** Embedding -> LSTM -> Dense -> Output\n",
    "- **Fully Connected Networks (Dense Networks):** Used for tabular data or simple regression/classification tasks.\n",
    "  - **Architecture:** Dense -> Dense -> Dense -> Output\n",
    "\n",
    "### 5. **Consider the Task's Requirements**\n",
    "\n",
    "- **Accuracy vs. Speed:** More layers can improve accuracy but might slow down the model. Find a balance based on your needs.\n",
    "- **Overfitting:** Adding too many layers can cause the model to overfit, meaning it performs well on training data but poorly on new data. Techniques like dropout layers or regularization can help prevent this.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Match Layers to Data:** Convolutional layers for images, recurrent layers for sequences, dense layers for general tasks.\n",
    "- **Start Simple:** Begin with a basic model and add complexity as needed.\n",
    "- **Experiment:** Try different combinations and see what works best for your task.\n",
    "- **Balance Complexity:** Avoid over-complicating your model unless necessary for the task.\n",
    "\n",
    "By understanding the nature of your task and data, you can decide on the right combination of layers to build an effective neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30d88aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 files belonging to 5 classes.\n",
      "Using 734 files for validation.\n",
      "val_ds before <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 5), dtype=tf.float32, name=None))>\n",
      "val_ds <_MapDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 5), dtype=tf.float32, name=None))>\n",
      "valid_size 734\n"
     ]
    }
   ],
   "source": [
    "val_ds = build_dataset('validation')\n",
    "print(\"val_ds before\",val_ds)\n",
    "valid_size = val_ds.cardinality().numpy()\n",
    "val_ds = val_ds.unbatch().batch(BATCH_SIZE)\n",
    "\n",
    "# def normalize_images(images, labels):\n",
    "#     return normalization_layer(images), labels\n",
    "# val_ds = val_ds.map(normalize_images)\n",
    "\n",
    "val_ds = val_ds.map(lambda images, \n",
    "                    labels: \n",
    "                    (normalization_layer(images), labels))\n",
    "\n",
    "print(\"val_ds\",val_ds)\n",
    "print(\"valid_size\", valid_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ce2c50",
   "metadata": {},
   "source": [
    "Now that you've provided the definition of the `build_dataset` function, let’s revisit the code in the context of how this function operates. Here’s how it all ties together:\n",
    "\n",
    "### `build_dataset` Function\n",
    "\n",
    "```python\n",
    "def build_dataset(subset):\n",
    "  return tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      data_dir,\n",
    "      validation_split=.20,\n",
    "      subset=subset,\n",
    "      label_mode=\"categorical\",\n",
    "      seed=123,\n",
    "      image_size=IMAGE_SIZE,\n",
    "      batch_size=1)\n",
    "```\n",
    "\n",
    "- **Purpose**: This function is used to create a dataset from images stored in a directory. It specifically handles splitting the dataset into training and validation subsets, resizing images, and setting the batch size.\n",
    "- **Parameters**:\n",
    "  - **`data_dir`**: Directory where the images are stored.\n",
    "  - **`validation_split=.20`**: 20% of the data is used for validation.\n",
    "  - **`subset=subset`**: Specifies whether to return the training or validation subset.\n",
    "  - **`label_mode=\"categorical\"`**: Labels are returned in one-hot encoded format, suitable for classification tasks.\n",
    "  - **`seed=123`**: Ensures the data split is consistent across different runs.\n",
    "  - **`image_size=IMAGE_SIZE`**: Resizes images to a standard size.\n",
    "  - **`batch_size=1`**: Initially loads the images in batches of 1 for fine-grained processing.\n",
    "\n",
    "### The Full Code with `build_dataset`\n",
    "\n",
    "```python\n",
    "val_ds = build_dataset(\"validation\")\n",
    "valid_size = val_ds.cardinality().numpy()\n",
    "val_ds = val_ds.unbatch().batch(BATCH_SIZE)\n",
    "val_ds = val_ds.map(lambda images, labels: (normalization_layer(images), labels))\n",
    "```\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Building the Dataset**:\n",
    "   ```python\n",
    "   val_ds = build_dataset(\"validation\")\n",
    "   ```\n",
    "   - This calls `build_dataset(\"validation\")`, which uses `tf.keras.preprocessing.image_dataset_from_directory` to load the validation subset of your images. The images are loaded in batches of 1, resized to `IMAGE_SIZE`, and labeled in a categorical format. The validation set will contain 20% of the total data.\n",
    "\n",
    "2. **Getting the Dataset Size**:\n",
    "   ```python\n",
    "   valid_size = val_ds.cardinality().numpy()\n",
    "   ```\n",
    "   - After loading the dataset, this line calculates the total number of images in the validation dataset by checking its cardinality (i.e., the total number of batches) and converting it to a Python number. This is useful for reporting, validation steps, or simply understanding the dataset's size.\n",
    "\n",
    "3. **Unbatching and Rebatching**:\n",
    "   ```python\n",
    "   val_ds = val_ds.unbatch().batch(BATCH_SIZE)\n",
    "   ```\n",
    "   - **Unbatching**: The `.unbatch()` method breaks down each batch (which currently contains only 1 image per batch) into individual images. This is done so that you can process each image individually if needed.\n",
    "   - **Rebatching**: After processing, the dataset is rebatched into larger batches defined by `BATCH_SIZE`. This is important for efficiency during validation, as processing large batches is more efficient than processing one image at a time.\n",
    "\n",
    "4. **Applying Normalization**:\n",
    "   ```python\n",
    "   val_ds = val_ds.map(lambda images, labels: (normalization_layer(images), labels))\n",
    "   ```\n",
    "   - This line applies a normalization step to the images using a `lambda` function within the `map` method. The `normalization_layer` scales the pixel values (likely from `[0, 255]` to `[0, 1]`), which is a common preprocessing step in image classification tasks.\n",
    "   - **Why Normalize?**: Normalizing the images helps the model learn better by ensuring that the input data has a consistent scale, which can improve both training stability and model accuracy.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **`build_dataset`**: Creates and loads the validation dataset, splitting it from the original data, resizing images, and loading them in batches of 1.\n",
    "- **`cardinality()`**: Calculates the number of images in the validation set.\n",
    "- **`unbatch()` and `batch()`**: Used to unbatch the dataset for individual processing and then rebatch it for efficient validation.\n",
    "- **Normalization**: Normalizes the images, ensuring they have a consistent range of pixel values, which is crucial for model performance.\n",
    "\n",
    "This sequence of operations prepares the validation dataset by ensuring it is correctly formatted, normalized, and batched, making it ready for use in model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c45465",
   "metadata": {},
   "source": [
    "<h1>Training dataset Vs Validation dataset</h1>\n",
    "\n",
    "In machine learning, the **training dataset** and **validation dataset** serve different but complementary purposes during the model development process. Here’s a breakdown of their differences:\n",
    "\n",
    "### 1. **Purpose**\n",
    "\n",
    "- **Training Dataset**:\n",
    "  - The training dataset is used to train the machine learning model. The model learns patterns, relationships, and features from this data by adjusting its parameters (weights and biases).\n",
    "  - The model iteratively processes this dataset, minimizing a loss function to improve its predictions.\n",
    "  \n",
    "- **Validation Dataset**:\n",
    "  - The validation dataset is used to tune the model's hyperparameters and evaluate its performance during training. It acts as an unseen set of data (relative to training) that the model uses to test its generalization to new data.\n",
    "  - The validation dataset helps in early stopping, tuning model architecture, and selecting the best model version to avoid overfitting to the training data.\n",
    "\n",
    "### 2. **Exposure to the Model**\n",
    "\n",
    "- **Training Dataset**:\n",
    "  - The model has direct access to the training data during the learning process. It repeatedly iterates over this dataset to improve its performance.\n",
    "  \n",
    "- **Validation Dataset**:\n",
    "  - The validation dataset is not used for learning or adjusting model parameters. Instead, it's used to test the model's performance at various points during training. The model doesn’t \"see\" this data during training, but it's evaluated on it to give insights into how well it may perform on unseen data.\n",
    "\n",
    "### 3. **Role in Model Development**\n",
    "\n",
    "- **Training Dataset**:\n",
    "  - Used to fit the model. The quality and quantity of the training dataset directly affect how well the model learns.\n",
    "  \n",
    "- **Validation Dataset**:\n",
    "  - Used to validate the model's performance during training. It helps identify issues like overfitting or underfitting. Overfitting occurs when the model performs well on the training data but poorly on the validation data, indicating that it has memorized the training data rather than generalizing from it.\n",
    "\n",
    "### 4. **Size and Composition**\n",
    "\n",
    "- **Training Dataset**:\n",
    "  - Generally larger than the validation dataset. It needs to be comprehensive enough to allow the model to learn all necessary patterns.\n",
    "  \n",
    "- **Validation Dataset**:\n",
    "  - Typically smaller than the training dataset, but it should still be representative of the data the model will encounter in the real world. Common practice is to allocate about 20-30% of the data for validation, though this can vary based on the specific problem and dataset size.\n",
    "\n",
    "### 5. **Effect on Model Tuning**\n",
    "\n",
    "- **Training Dataset**:\n",
    "  - Directly influences the model’s learning process and the resulting parameters.\n",
    "  \n",
    "- **Validation Dataset**:\n",
    "  - Influences model selection and tuning. For example, it helps in selecting the number of layers in a neural network, the learning rate, and other hyperparameters.\n",
    "\n",
    "### Example in Context\n",
    "\n",
    "- **Training**: If you're developing a model to classify images of cats and dogs, you would use thousands of labeled images of cats and dogs to train the model. The model would adjust its internal parameters based on how well it predicts the labels for these images.\n",
    "- **Validation**: Simultaneously, a separate set of images that the model hasn’t been trained on (validation dataset) would be used to check how well the model is doing. If the model performs well on the training data but poorly on the validation data, it might indicate overfitting.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- The **training dataset** is for teaching the model, while the **validation dataset** is for evaluating how well the model is likely to perform on new, unseen data.\n",
    "- The validation dataset helps ensure that the model generalizes well beyond the specific examples it was trained on, helping prevent overfitting.\n",
    "\n",
    "These two datasets play distinct but essential roles in developing a robust and reliable machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7674dced",
   "metadata": {},
   "source": [
    "<H2>Building Model</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8101ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_fine_tuning = False\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
    "    hub.KerasLayer(model_handle, trainable=do_fine_tuning),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(len(class_names), kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "])\n",
    "\n",
    "model.build((None,)+IMAGE_SIZE+(3,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c72d10",
   "metadata": {},
   "source": [
    "Let's break down the code step by step to understand what it does:\n",
    "\n",
    "### Code Explanation\n",
    "\n",
    "#### 1. **InputLayer**\n",
    "   - **`tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,))`**:\n",
    "     - **Purpose**: This layer defines the input shape for the model. `IMAGE_SIZE` is typically a tuple like `(224, 224)` representing the width and height of the images. Adding `(3,)` indicates that the images have 3 color channels (i.e., RGB).\n",
    "     - **Explanation**: This layer ensures that all images fed into the model have the correct shape.\n",
    "\n",
    "#### 2. **KerasLayer**\n",
    "   - **`hub.KerasLayer(model_handle, trainable=do_fine_tuning)`**:\n",
    "     - **Purpose**: This layer wraps a pre-trained model from TensorFlow Hub. The `model_handle` is a string that points to the specific model you want to use (e.g., MobileNet, EfficientNet).\n",
    "     - **`trainable=do_fine_tuning`**:\n",
    "       - **`True`**: If `do_fine_tuning` is `True`, this allows the weights of the pre-trained model to be updated during training.\n",
    "       - **`False`**: If `do_fine_tuning` is `False`, the pre-trained model's weights are frozen and only the new layers you add will be trained.\n",
    "     - **Explanation**: This is where transfer learning happens. The model can use powerful features learned by a model pre-trained on a large dataset like ImageNet.\n",
    "\n",
    "#### 3. **Dropout Layer**\n",
    "   - **`tf.keras.layers.Dropout(rate=0.2)`**:\n",
    "     - **Purpose**: This layer helps prevent overfitting by randomly setting 20% of the input units to zero during each update during training time. Dropout is a regularization technique.\n",
    "     - **Explanation**: Dropout makes the model more robust by ensuring it doesn’t rely too heavily on specific nodes, forcing it to learn more generalized features.\n",
    "\n",
    "#### 4. **Dense Layer**\n",
    "   - **`tf.keras.layers.Dense(len(class_names), kernel_regularizer=tf.keras.regularizers.l2(0.0001))`**:\n",
    "     - **Purpose**: This is the final layer in the model, which produces the output. The number of units in this layer equals the number of classes in the classification task (e.g., if you have 10 classes, `len(class_names)` would be 10).\n",
    "     - **`kernel_regularizer=tf.keras.regularizers.l2(0.0001)`**:\n",
    "       - **Purpose**: This adds L2 regularization to the Dense layer's weights, penalizing large weights and helping to reduce overfitting.\n",
    "     - **Explanation**: This layer takes the feature maps from the pre-trained model and maps them to the output classes.\n",
    "\n",
    "### Model Build\n",
    "```python\n",
    "model.build((None,)+IMAGE_SIZE+(3,))\n",
    "```\n",
    "\n",
    "- **Purpose**: The `build` method initializes the model with a specified input shape. `(None,)+IMAGE_SIZE+(3,)` is the shape of the input data where `None` represents the batch size (which can be any value), `IMAGE_SIZE` is the image dimensions (e.g., `(224, 224)`), and `3` represents the RGB color channels.\n",
    "- **Explanation**: This ensures that the model is properly initialized and ready for training with inputs of the specified shape.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Model Structure**: This model starts with an input layer, followed by a pre-trained model from TensorFlow Hub, a dropout layer for regularization, and finally a dense layer for classification.\n",
    "- **Fine-Tuning**: The model allows for fine-tuning depending on the value of `do_fine_tuning`. If fine-tuning is disabled, the pre-trained model's weights are frozen, and only the new layers are trained.\n",
    "- **Dropout and Regularization**: The model includes a dropout layer to prevent overfitting and L2 regularization in the dense layer for the same reason.\n",
    "\n",
    "This setup is typical for transfer learning, where you leverage the power of a pre-trained model and adapt it to your specific classification task with minimal additional training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5542b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
